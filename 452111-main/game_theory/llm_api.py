"""
ç»Ÿä¸€ LLM API è°ƒç”¨æ¥å£
Unified LLM API Interface

æ”¯æŒ: OpenAI / Gemini / DeepSeek / æœ¬åœ°Ollama
"""

import os
import json
from typing import Optional, Dict, List

import requests

# å…¨å±€å…±äº«çš„ Session å®ä¾‹ï¼Œç”¨äºè·¨çº¿ç¨‹å¤ç”¨ TCP è¿æ¥
# requests.Session æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¯ä»¥åœ¨å¤šçº¿ç¨‹ä¸­å…±äº«ä½¿ç”¨
_shared_session: requests.Session = None


def get_shared_session() -> requests.Session:
    """
    è·å–å…¨å±€å…±äº«çš„ requests.Session

    ä¼˜åŒ–ç‚¹ï¼š
    - å¤ç”¨ TCP è¿æ¥ï¼Œé¿å…æ¯æ¬¡è¯·æ±‚éƒ½è¿›è¡Œ TCP æ¡æ‰‹å’Œ SSL éªŒè¯
    - çº¿ç¨‹å®‰å…¨ï¼Œå¯åœ¨ ThreadPoolExecutor ä¸­å®‰å…¨ä½¿ç”¨
    - å•ä¾‹æ¨¡å¼ï¼Œæ•´ä¸ªè¿›ç¨‹åªç»´æŠ¤ä¸€ä¸ªè¿æ¥æ± 
    """
    global _shared_session
    if _shared_session is None:
        _shared_session = requests.Session()
        # é…ç½®è¿æ¥æ± å¤§å°ï¼Œé€‚åº”å¹¶å‘ API è¯·æ±‚
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=100,  # è¿æ¥æ± æ•°é‡
            pool_maxsize=100,      # æ¯ä¸ªè¿æ¥æ± çš„æœ€å¤§è¿æ¥æ•°
            max_retries=3          # è‡ªåŠ¨é‡è¯•æ¬¡æ•°
        )
        _shared_session.mount('http://', adapter)
        _shared_session.mount('https://', adapter)
    return _shared_session

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = os.path.join(os.path.dirname(__file__), "llm_config.json")

DEFAULT_CONFIG = {
    "default_provider": "deepseek",
    
    "openai": {
        "api_key": "",
        "base_url": "https://hiapi.online/v1",
        "model": "gpt-4o-mini",
    },
    "gemini": {
        "api_key": "",
        "base_url": "https://generativelanguage.googleapis.com",
        "model": "gemini-2.0-flash",
    },
    "deepseek": {
        "api_key": "",
        "base_url": "https://api.deepseek.com",
        "model": "deepseek-chat",
    },

}


def load_config() -> Dict:
    """åŠ è½½é…ç½®"""
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    save_config(DEFAULT_CONFIG)
    return DEFAULT_CONFIG


def save_config(config: Dict):
    """ä¿å­˜é…ç½®"""
    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)


def get_api_key(provider: str) -> str:
    """è·å– API Keyï¼ˆä¼˜å…ˆç¯å¢ƒå˜é‡ï¼‰"""
    env_keys = {
        "openai": "OPENAI_API_KEY",
        "gemini": "GEMINI_API_KEY",
        "deepseek": "DEEPSEEK_API_KEY",
    }
    if provider in env_keys:
        env_val = os.environ.get(env_keys[provider])
        if env_val:
            return env_val
    config = load_config()
    return config.get(provider, {}).get("api_key", "")


class LLMClient:
    """
    ç»Ÿä¸€ LLM å®¢æˆ·ç«¯

    Example:
        llm = LLMClient()  # ä½¿ç”¨é»˜è®¤ provider
        llm = LLMClient(provider="openai")
        response = llm.chat("ä½ å¥½")

    ä¼˜åŒ–ï¼šä½¿ç”¨ requests.Session å¤ç”¨ TCP è¿æ¥ï¼Œ
    é¿å…æ¯æ¬¡è¯·æ±‚éƒ½è¿›è¡Œ TCP æ¡æ‰‹å’Œ SSL éªŒè¯ï¼ˆèŠ‚çœçº¦ 0.2~0.5ç§’/è¯·æ±‚ï¼‰
    """

    def __init__(self, provider: str = None, session: requests.Session = None):
        self.config = load_config()
        provider = provider or self.config.get("default_provider", "deepseek")
        self.provider = provider
        # é»˜è®¤ä½¿ç”¨å…¨å±€å…±äº«çš„ Sessionï¼Œå¤ç”¨ TCP è¿æ¥
        # ä¹Ÿå¯ä¼ å…¥è‡ªå®šä¹‰ session
        self.session = session or get_shared_session()
        
    def chat(self,
             prompt: str,
             system_prompt: str = None,
             temperature: float = 0.7,
             max_tokens: int = 500) -> str:
        """å‘é€èŠå¤©è¯·æ±‚"""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        if self.provider == "gemini":
            return self._call_gemini(messages, temperature, max_tokens)
        elif self.provider == "ollama":
            return self._call_ollama(messages, temperature, max_tokens)
        else:
            return self._call_openai_compatible(messages, temperature, max_tokens)
    
    def _call_openai_compatible(self, messages: List[Dict], temperature: float, max_tokens: int) -> str:
        """è°ƒç”¨ OpenAI å…¼å®¹ API (OpenAI/DeepSeek/ä¸­è½¬ç«™)"""
        provider_config = self.config.get(self.provider, {})
        api_key = get_api_key(self.provider)
        base_url = provider_config.get("base_url", "https://api.openai.com/v1")
        model = provider_config.get("model", "gpt-4o-mini")

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        data = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        try:
            resp = self.session.post(f"{base_url}/chat/completions", headers=headers, json=data, timeout=60)
            resp.raise_for_status()
            return resp.json()["choices"][0]["message"]["content"]
        except Exception as e:
            return f"[API Error: {e}]"
    
    def _call_gemini(self, messages: List[Dict], temperature: float, max_tokens: int) -> str:
        """è°ƒç”¨ Gemini API"""
        api_key = get_api_key("gemini")
        model = self.config.get("gemini", {}).get("model", "gemini-1.5-flash")

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º Gemini æ ¼å¼
        contents = []
        system_instruction = None
        for msg in messages:
            if msg["role"] == "system":
                system_instruction = msg["content"]
            elif msg["role"] == "user":
                contents.append({"role": "user", "parts": [{"text": msg["content"]}]})
            elif msg["role"] == "assistant":
                contents.append({"role": "model", "parts": [{"text": msg["content"]}]})

        data = {
            "contents": contents,
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens,
            }
        }
        if system_instruction:
            data["systemInstruction"] = {"parts": [{"text": system_instruction}]}

        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"

        try:
            resp = self.session.post(url, json=data, timeout=60)
            resp.raise_for_status()
            return resp.json()["candidates"][0]["content"]["parts"][0]["text"]
        except Exception as e:
            return f"[Gemini Error: {e}]"
    
    def _call_ollama(self, messages: List[Dict], temperature: float, max_tokens: int) -> str:
        """è°ƒç”¨æœ¬åœ° Ollama"""
        ollama_config = self.config.get("ollama", {})
        base_url = ollama_config.get("base_url", "http://localhost:11434")
        model = ollama_config.get("model", "llama3")

        data = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": {"temperature": temperature, "num_predict": max_tokens}
        }

        try:
            resp = self.session.post(f"{base_url}/api/chat", json=data, timeout=120)
            resp.raise_for_status()
            return resp.json()["message"]["content"]
        except Exception as e:
            return f"[Ollama Error: {e}]"
    
    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            response = self.chat("Say OK", max_tokens=10)
            return not response.startswith("[") and len(response) > 0
        except Exception:
            return False


def chat(prompt: str, provider: str = None, **kwargs) -> str:
    """å¿«é€ŸèŠå¤©æ¥å£"""
    client = LLMClient(provider=provider)
    return client.chat(prompt, **kwargs)


def setup_wizard():
    """é…ç½®å‘å¯¼"""
    print("\n" + "="*50)
    print("ğŸ”§ LLM API é…ç½®å‘å¯¼")
    print("="*50)
    
    config = load_config()
    
    print("\nå¯ç”¨ Provider:")
    print("  1. deepseek  - Â¥1/ç™¾ä¸‡tokenï¼Œæœ€ä¾¿å®œ (æ¨è)")
    print("  2. openai    - GPT-4o-mini")
    print("  3. gemini    - Gemini 1.5 Flash")
    print("  4. ollama    - æœ¬åœ°æ¨¡å‹ï¼Œå…è´¹")

    choice = input("\né€‰æ‹© (1-4) [1]: ").strip() or "1"
    provider = {"1": "deepseek", "2": "openai", "3": "gemini", "4": "ollama"}.get(choice, "deepseek")
    config["default_provider"] = provider
    
    if provider != "ollama":
        api_key = input(f"\nè¾“å…¥ {provider.upper()} API Key: ").strip()
        if api_key:
            config[provider]["api_key"] = api_key
    
    save_config(config)
    print(f"\nâœ… é…ç½®å·²ä¿å­˜")
    
    print("\næµ‹è¯•è¿æ¥...")
    client = LLMClient(provider=provider)
    if client.test_connection():
        print("âœ… è¿æ¥æˆåŠŸ!")
    else:
        print("âŒ è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "setup":
        setup_wizard()
    else:
        print("è¿è¡Œ python llm_api.py setup è¿›è¡Œé…ç½®")
